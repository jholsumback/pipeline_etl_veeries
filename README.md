# Pipeline ETL - Dados Portu√°rios (Paranagu√° e Santos)

Este projeto implementa um pipeline **ETL (Extract, Transform, Load)** para coletar, tratar, analisar e armazenar dados de movimenta√ß√£o de navios dos portos de **Paranagu√°** e **Santos**.

Todo o processo foi organizado no padr√£o **Medallion Architecture** (camadas **bronze**, **silver** e **gold**), o que facilita a manuten√ß√£o, rastreabilidade e confiabilidade dos dados.  
Os resultados finais tamb√©m foram carregados em uma **tabela SQL**, que permite consultas e visualiza√ß√£o de forma mais pr√°tica.

---

## Fluxo do Processo

1. **Extra√ß√£o (Bronze)**  
   - Faz a requisi√ß√£o HTTP para as p√°ginas dos portos.  
   - Verifica se a resposta foi bem-sucedida.  
   - Faz o parsing do HTML com **BeautifulSoup**.  
   - L√™ as tabelas da p√°gina com **pandas.read_html()**.  
   - Salva as tabelas de interesse como arquivos **CSV** na pasta `data/bronze`, mantendo o formato **id√™ntico ao site**.

2. **Transforma√ß√£o (Silver)**  
   - L√™ os arquivos CSV da camada bronze.  
   - Pula a primeira linha (`skiprows=1`) pois cont√©m cabe√ßalhos extras.  
   - Mant√©m apenas as colunas relevantes para an√°lise.  
   - Concatena os dados (atracados e despachados).  
   - Salva um **√∫nico CSV normalizado** na pasta `data/silver`.

3. **Normaliza√ß√£o e Enriquecimento (Gold)**  
   - L√™ os arquivos da camada silver para ambos os portos.  
   - Adiciona a coluna `porto` para identificar a origem.  
   - Unifica colunas equivalentes (por exemplo, `Sentido` / `Opera√ß Operat`) para padroniza√ß√£o.    
   - Adiciona a coluna `data_execucao` com a data/hora da execu√ß√£o do script.  
   - Exclui colunas redundantes e ordena as colunas para facilitar a an√°lise.  
   - Salva o resultado final como **CSV** na pasta `data/gold`.

4. **Carga no Banco de Dados (SQL)**  
   - Os dados processados s√£o carregados em uma tabela SQL, definida no arquivo de script da pasta `database_scripts`.  
   - Foi criada uma **√∫nica tabela consolidada** no banco, facilitando a visualiza√ß√£o e as an√°lises posteriores.

---

## üìÇ Estrutura do Projeto

navio_pipeline/

data:

    - bronze/ CSVs extra√≠dos do site (brutos)
    - silver/ CSVs normalizados
    - gold/ CSV final pronto para an√°lises
database:
    
    - database.py/ Scripts e conex√£o com o banco/Configura√ß√£o da conex√£o com SQL

database_scripts:

    - script.sql/ Script para criar tabela no banco

root:

    - main.py/ Arquivo principal que orquestra as fun√ß√µes

medallion:

    - bronze.py/ L√≥gica de extra√ß√£o
    - silver.py/ L√≥gica de limpeza e padroniza√ß√£o
    - gold.py/ L√≥gica de unifica√ß√£o e carga

- requirements.txt: Depend√™ncias do projeto
- venv: Ambiente virtual 
- README.md: Este arquivo

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.12.10**
- [Pandas](https://pandas.pydata.org/)
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)
- [Requests](https://requests.readthedocs.io/)
- [PostgreSQL](https://www.postgresql.org/)
- [SQLAlchemy](https://www.sqlalchemy.org/)
- [Schedule](https://pypi.org/project/schedule/)
- VS Code como ambiente de desenvolvimento

---

## ‚öôÔ∏è Instala√ß√£o

1. Clone este reposit√≥rio:
```bash
git clone https://github.com/jholsumback/pipeline_etl_veeries.git
cd pipeline_etl_veeries
```

2. Crie e ative o ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate   # Linux / Mac
venv\Scripts\activate      # Windows
```
3. Instale as depend√™ncias:
```bash
pip install -r requirements.txt
```
4. Configure o acesso ao banco no aquivo:
- database/database.py

## Execu√ß√£o do pipeline
- Execute o arquivo main.py para rodar todo o fluxo:
```bash
python main.py
```
üìà Observa√ß√µes Importantes

As sa√≠das ser√£o geradas automaticamente nas pastas bronze ‚Üí silver ‚Üí gold e no banco de dados.

A consolida√ß√£o final foi feita em uma √∫nica tabela SQL, o que facilita consultas e visualiza√ß√£o.

As camadas bronze, silver e gold permitem rastrear cada etapa do pipeline.

A padroniza√ß√£o e a cria√ß√£o da coluna data_execucao garantem consist√™ncia hist√≥rica dos dados.

Optei por manter a coluna com a unidade original em MOVs (movimentos) em vez de convert√™-la para toneladas (Tons), porque cada uma representa uma m√©trica distinta:

MOVs indicam o n√∫mero de opera√ß√µes de embarque ou desembarque realizadas.

Tons representam o peso da carga movimentada.

Se eu convertesse os MOVs para toneladas, misturaria duas naturezas de informa√ß√£o diferentes contagem de opera√ß√µes vs. peso transportado, o que poderia causar confus√£o na interpreta√ß√£o dos dados.

Ao manter a m√©trica original, a tabela fica mais clara e coerente, permitindo an√°lises separadas: por quantidade de opera√ß√µes (MOVs) ou por volume/peso (Tons), de acordo com a necessidade.

**Agendamento autom√°tico do pipeline**
O projeto utiliza a biblioteca schedule para automatizar a execu√ß√£o di√°ria do pipeline ETL. O seguinte trecho de c√≥digo garante que todo o processo de extra√ß√£o, transforma√ß√£o, agrega√ß√£o e carga dos dados seja executado automaticamente todos os dias √†s 09:00 (no arquivo main.py). Isso permite que o pipeline funcione de forma autom√°tica, sem necessidade de execu√ß√£o manual.

**Dicion√°rio tabela SQL:**

- Coluna **sentido**: indica se a opera√ß√£o √© de exporta√ß√£o ou importa√ß√£o (IMP/EXP), e Embarque e Desembarque (EMB/DESC).
- Coluna **produto**: especifica o tipo de produto que est√° sendo transportado.
- Coluna **navio**: informa o nome ou tipo do navio respons√°vel pelo transporte.
- Coluna **porto**: identifica o porto de origem dos dados.
- Coluna **data_execucao**: registra a data e hora em que o script foi executado.
- Coluna **saldo total**: indica o volume total efetivamente carregado.
- Coluna **chegada**: indica a data e hora de chegada do navio ao porto.
